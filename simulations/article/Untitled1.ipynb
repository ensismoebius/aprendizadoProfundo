{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "976e6813-9e47-4bf4-84b5-c4313bae8ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import snntorch as snn\n",
    "from snntorch import surrogate\n",
    "from data import EEGDataset\n",
    "from torch import nn, save, load\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "026c023d-2cb2-485e-82e2-25dba692325f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class EEGDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        \n",
    "        ####################\n",
    "        ### Loading data ###\n",
    "        ####################\n",
    "        \n",
    "        # '/home/ensismoebius/Documentos/UNESP/doutorado/databases/Base de Datos Habla Imaginada/S01/S01_EEG.mat',\n",
    "        mat = loadmat(path, struct_as_record=True, squeeze_me=True, mat_dtype=False)\n",
    "        \n",
    "        #################################\n",
    "        ### Setting up the properties ###\n",
    "        #################################\n",
    "        \n",
    "        self.estimuli = {\n",
    "                1 : \"A\",\n",
    "                2 : \"E\",\n",
    "                3 : \"I\",\n",
    "                4 : \"O\",\n",
    "                5 : \"U\",\n",
    "                6 : \"Arriba\",\n",
    "                7 : \"Abajo\",\n",
    "                8 : \"Adelante\",\n",
    "                9 : \"Atrás\",\n",
    "                10 : \"Derecha\",\n",
    "                11 : \"Izquierda\"\n",
    "            }\n",
    "        \n",
    "        # Modalities\n",
    "        self.modalities = {\n",
    "            1 : \"Imaginada\",\n",
    "            2 : \"Falada\" \n",
    "        }\n",
    "        \n",
    "        # Artfacts\n",
    "        self.artfacts = {\n",
    "            -1 : \"Indiferente\",  \n",
    "            1 : \"Com artefato\", \n",
    "            2 : \"Sem artefato\" \n",
    "        }\n",
    "\n",
    "        self.dataframe = pd.DataFrame(mat['EEG'])\n",
    "\n",
    "        self._joinIntoArray(0, 4096, 'F3', self.dataframe)\n",
    "        self._joinIntoArray(0, 4096, 'F4', self.dataframe)\n",
    "        self._joinIntoArray(0, 4096, 'C3', self.dataframe)\n",
    "        self._joinIntoArray(0, 4096, 'C4', self.dataframe)\n",
    "        self._joinIntoArray(0, 4096, 'P3', self.dataframe)\n",
    "        self._joinIntoArray(0, 4096, 'P4', self.dataframe)\n",
    "        \n",
    "        self._joinIntoValue(0, 1, 'Modalidade', self.dataframe)\n",
    "        self._joinIntoValue(0, 1, 'Estímulo', self.dataframe)\n",
    "        self._joinIntoValue(0, 1, 'Artefatos', self.dataframe)\n",
    "        \n",
    "        self.filteredData = self.dataframe[(self.dataframe['Modalidade'] == 1) & (self.dataframe['Artefatos'] == 1)]\n",
    "        \n",
    "        self.labels = self.filteredData['Estímulo'].values\n",
    "        self.data = self.filteredData[['F3','F4','C3','C4','P3','P4']].values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        sample_data =  torch.tensor(self.data[idx].tolist(),dtype=torch.float) # Convert numpy arrays to tensors\n",
    "        sample_label = torch.tensor(self.labels[idx].tolist(),dtype=torch.float)  # Ensure labels are also tensors\n",
    "        return sample_data, sample_label\n",
    "\n",
    "    # Auxiliary methods            \n",
    "    def _joinIntoArray(self, start_col, end_col, newColumn, dataframe):\n",
    "        cols_to_join = dataframe.iloc[:, start_col:end_col].columns\n",
    "        dataframe[newColumn] = dataframe[cols_to_join].apply(lambda x: np.array(pd.to_numeric(x, errors='coerce')), axis=1)\n",
    "        dataframe.drop(cols_to_join, axis=1, inplace=True)\n",
    "    \n",
    "    def _joinIntoValue(self, start_col, end_col, newColumn, dataframe):\n",
    "        cols_to_join = dataframe.iloc[:, start_col:end_col].columns\n",
    "        dataframe[newColumn] = dataframe[cols_to_join].apply(lambda x: pd.to_numeric(x, errors='coerce'), axis=1)\n",
    "        dataframe.drop(cols_to_join, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03ab0cfa-1a7d-4480-a93c-a0317b55e2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensor_size_after_conv(input_size, convolution_kernel_size):\n",
    "    return 1 +(input_size - convolution_kernel_size)\n",
    "\n",
    "def get_tensor_size_after_maxpool(input_size, poll_kernel_size):\n",
    "    return math.floor(input_size / poll_kernel_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79b20b41-69a8-4e3b-a93d-c51cdc160f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "data_path='/home/ensismoebius/Documentos/UNESP/doutorado/databases/Base de Datos Habla Imaginada/S01/S01_EEG.mat'\n",
    "device = torch.device(\"cpu\")\n",
    "input_channels = 6  # number of EEG channels\n",
    "input_timepoints = 4096  # number of time points in each EEG sample\n",
    "\n",
    "beta = 0.9  # neuron decay rate \n",
    "spike_grad = surrogate.fast_sigmoid() # fast sigmoid surrogate gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b1154f8-e53d-4cbb-b355-22277b0b7d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "eegDataset = EEGDataset(data_path)\n",
    "train_loader = DataLoader(eegDataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(eegDataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbcd765f-7280-4efa-a32e-9436fa573d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "######## Calculates the size of the feature vector #########\n",
    "############################################################\n",
    "\n",
    "# Calculate the size of the features after the 1st conv and pool layers\n",
    "feature_size = get_tensor_size_after_conv(input_timepoints,5)\n",
    "feature_size = get_tensor_size_after_maxpool(feature_size,2)\n",
    "# Calculate the size of the features after the 2nd conv and pool layers\n",
    "feature_size = get_tensor_size_after_conv(feature_size,5)\n",
    "feature_size = get_tensor_size_after_maxpool(feature_size,2)\n",
    "# Calculate the size of the features after the flatten\n",
    "feature_size = feature_size * 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9cf3d1f-b341-4193-9b79-93fab8f702e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32672"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4cc3c6f-aab5-4715-bebd-a044c99692ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "###### Creates the model #######\n",
    "################################\n",
    "\n",
    "#  Initialize  SNN\n",
    "net = nn.Sequential(\n",
    "    nn.Conv1d(in_channels=input_channels, out_channels=16, kernel_size=5),\n",
    "    nn.MaxPool1d(kernel_size=2),\n",
    "    \n",
    "    nn.Conv1d(in_channels=16, out_channels=32, kernel_size=5),\n",
    "    nn.MaxPool1d(kernel_size=2),\n",
    "\n",
    "    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(feature_size, 128),\n",
    "    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True)\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a9a3540-4145-455e-99f3-788fa322c8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snntorch import utils \n",
    "\n",
    "def forward_pass(net, data, num_steps):  \n",
    "  spk_rec = [] # record spikes over time\n",
    "  utils.reset(net)  # reset/initialize hidden states for all LIF neurons in net\n",
    "\n",
    "  for step in range(num_steps): # loop over time\n",
    "      spk_out, mem_out = net(data) # one time step of the forward-pass\n",
    "      spk_rec.append(spk_out) # record spikes\n",
    "  \n",
    "  return torch.stack(spk_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb1a114e-0d86-484f-96d8-e153c6262c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snntorch.functional as SF\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=2e-3, betas=(0.9, 0.999))\n",
    "loss_fn = SF.mse_count_loss(correct_rate=0.8, incorrect_rate=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ea5c80e-d9b4-43d3-a06c-33b4f122e97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25615/3260686380.py:71: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  sample_data =  torch.tensor(self.data[idx].tolist(),dtype=torch.float) # Convert numpy arrays to tensors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Iteration 0 \n",
      "Train Loss: 1.00\n",
      "Accuracy: 0.00%\n",
      "\n",
      "Epoch 0, Iteration 25 \n",
      "Train Loss: 10.02\n",
      "Accuracy: 25.00%\n",
      "\n",
      "Epoch 0, Iteration 50 \n",
      "Train Loss: 9.96\n",
      "Accuracy: 25.00%\n",
      "\n",
      "Epoch 1, Iteration 0 \n",
      "Train Loss: 9.88\n",
      "Accuracy: 25.00%\n",
      "\n",
      "Epoch 1, Iteration 25 \n",
      "Train Loss: 9.55\n",
      "Accuracy: 25.00%\n",
      "\n",
      "Epoch 1, Iteration 50 \n",
      "Train Loss: 9.03\n",
      "Accuracy: 0.00%\n",
      "\n",
      "Epoch 2, Iteration 0 \n",
      "Train Loss: 8.76\n",
      "Accuracy: 25.00%\n",
      "\n",
      "Epoch 2, Iteration 25 \n",
      "Train Loss: 8.85\n",
      "Accuracy: 0.00%\n",
      "\n",
      "Epoch 2, Iteration 50 \n",
      "Train Loss: 9.09\n",
      "Accuracy: 0.00%\n",
      "\n",
      "Epoch 3, Iteration 0 \n",
      "Train Loss: 8.62\n",
      "Accuracy: 0.00%\n",
      "\n",
      "Epoch 3, Iteration 25 \n",
      "Train Loss: 5.51\n",
      "Accuracy: 0.00%\n",
      "\n",
      "Epoch 3, Iteration 50 \n",
      "Train Loss: 2.81\n",
      "Accuracy: 0.00%\n",
      "\n",
      "Epoch 4, Iteration 0 \n",
      "Train Loss: 2.87\n",
      "Accuracy: 0.00%\n",
      "\n",
      "Epoch 4, Iteration 25 \n",
      "Train Loss: 2.88\n",
      "Accuracy: 0.00%\n",
      "\n",
      "Epoch 4, Iteration 50 \n",
      "Train Loss: 2.82\n",
      "Accuracy: 25.00%\n",
      "\n",
      "Epoch 5, Iteration 0 \n",
      "Train Loss: 2.88\n",
      "Accuracy: 0.00%\n",
      "\n",
      "Epoch 5, Iteration 25 \n",
      "Train Loss: 2.90\n",
      "Accuracy: 0.00%\n",
      "\n",
      "Epoch 5, Iteration 50 \n",
      "Train Loss: 2.82\n",
      "Accuracy: 0.00%\n",
      "\n",
      "Epoch 6, Iteration 0 \n",
      "Train Loss: 2.79\n",
      "Accuracy: 0.00%\n",
      "\n",
      "Epoch 6, Iteration 25 \n",
      "Train Loss: 2.70\n",
      "Accuracy: 25.00%\n",
      "\n",
      "Epoch 6, Iteration 50 \n",
      "Train Loss: 2.64\n",
      "Accuracy: 0.00%\n",
      "\n",
      "Epoch 7, Iteration 0 \n",
      "Train Loss: 2.58\n",
      "Accuracy: 25.00%\n",
      "\n",
      "Epoch 7, Iteration 25 \n",
      "Train Loss: 2.61\n",
      "Accuracy: 0.00%\n",
      "\n",
      "Epoch 7, Iteration 50 \n",
      "Train Loss: 2.76\n",
      "Accuracy: 0.00%\n",
      "\n",
      "Epoch 8, Iteration 0 \n",
      "Train Loss: 2.70\n",
      "Accuracy: 25.00%\n",
      "\n",
      "Epoch 8, Iteration 25 \n",
      "Train Loss: 2.76\n",
      "Accuracy: 0.00%\n",
      "\n",
      "Epoch 8, Iteration 50 \n",
      "Train Loss: 2.82\n",
      "Accuracy: 0.00%\n",
      "\n",
      "Epoch 9, Iteration 0 \n",
      "Train Loss: 2.93\n",
      "Accuracy: 25.00%\n",
      "\n",
      "Epoch 9, Iteration 25 \n",
      "Train Loss: 2.88\n",
      "Accuracy: 0.00%\n",
      "\n",
      "Epoch 9, Iteration 50 \n",
      "Train Loss: 2.82\n",
      "Accuracy: 25.00%\n",
      "\n",
      "Epoch 10, Iteration 0 \n",
      "Train Loss: 2.82\n",
      "Accuracy: 25.00%\n",
      "\n",
      "Epoch 10, Iteration 25 \n",
      "Train Loss: 2.90\n",
      "Accuracy: 0.00%\n",
      "\n",
      "Epoch 10, Iteration 50 \n",
      "Train Loss: 2.82\n",
      "Accuracy: 25.00%\n",
      "\n",
      "Epoch 11, Iteration 0 \n",
      "Train Loss: 2.82\n",
      "Accuracy: 25.00%\n",
      "\n",
      "Epoch 11, Iteration 25 \n",
      "Train Loss: 2.88\n",
      "Accuracy: 0.00%\n",
      "\n",
      "Epoch 11, Iteration 50 \n",
      "Train Loss: 2.82\n",
      "Accuracy: 25.00%\n",
      "\n",
      "Epoch 12, Iteration 0 \n",
      "Train Loss: 2.93\n",
      "Accuracy: 25.00%\n",
      "\n",
      "Epoch 12, Iteration 25 \n",
      "Train Loss: 2.99\n",
      "Accuracy: 0.00%\n",
      "\n",
      "Epoch 12, Iteration 50 \n",
      "Train Loss: 2.88\n",
      "Accuracy: 0.00%\n",
      "\n",
      "Epoch 13, Iteration 0 \n",
      "Train Loss: 2.82\n",
      "Accuracy: 25.00%\n",
      "\n",
      "Epoch 13, Iteration 25 \n",
      "Train Loss: 2.88\n",
      "Accuracy: 0.00%\n",
      "\n",
      "Epoch 13, Iteration 50 \n",
      "Train Loss: 2.82\n",
      "Accuracy: 25.00%\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m loss_val \u001b[38;5;241m=\u001b[39m loss_fn(spk_rec, targets) \u001b[38;5;66;03m# loss calculation\u001b[39;00m\n\u001b[1;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad() \u001b[38;5;66;03m# null gradients\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[43mloss_val\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# calculate gradients\u001b[39;00m\n\u001b[1;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep() \u001b[38;5;66;03m# update weights\u001b[39;00m\n\u001b[1;32m     19\u001b[0m loss_hist\u001b[38;5;241m.\u001b[39mappend(loss_val\u001b[38;5;241m.\u001b[39mitem()) \u001b[38;5;66;03m# store loss\u001b[39;00m\n",
      "File \u001b[0;32m~/.venvTorch/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venvTorch/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venvTorch/lib/python3.11/site-packages/torch/autograd/function.py:276\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBackwardCFunction\u001b[39;00m(_C\u001b[38;5;241m.\u001b[39m_FunctionBase, FunctionCtx, _HookMixin):\n\u001b[0;32m--> 276\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;66;03m# _forward_cls is defined by derived class\u001b[39;00m\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;66;03m# The user should define either backward or vjp but never both.\u001b[39;00m\n\u001b[1;32m    279\u001b[0m         backward_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cls\u001b[38;5;241m.\u001b[39mbackward  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    280\u001b[0m         vjp_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cls\u001b[38;5;241m.\u001b[39mvjp  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 30 # run for 1 epoch - each data sample is seen only once\n",
    "num_steps = 25  # run for 25 time steps \n",
    "\n",
    "loss_hist = [] # record loss over iterations \n",
    "acc_hist = [] # record accuracy over iterations\n",
    "\n",
    "# training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data, targets) in enumerate(iter(train_loader)):\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        net.train() \n",
    "        spk_rec = forward_pass(net, data, num_steps) # forward-pass\n",
    "        loss_val = loss_fn(spk_rec, targets) # loss calculation\n",
    "        optimizer.zero_grad() # null gradients\n",
    "        loss_val.backward() # calculate gradients\n",
    "        optimizer.step() # update weights\n",
    "        loss_hist.append(loss_val.item()) # store loss\n",
    "\n",
    "        # print every 25 iterations\n",
    "        if i % 25 == 0:\n",
    "          print(f\"Epoch {epoch}, Iteration {i} \\nTrain Loss: {loss_val.item():.2f}\")\n",
    "\n",
    "          # check accuracy on a single batch\n",
    "          acc = SF.accuracy_rate(spk_rec, targets) \n",
    "          acc_hist.append(acc)\n",
    "          print(f\"Accuracy: {acc * 100:.2f}%\\n\")\n",
    "        \n",
    "        # uncomment for faster termination\n",
    "        # if i == 150:\n",
    "        #     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560c2b22-25b5-446c-9081-540be2f9bede",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
