@InProceedings{10.1007/978-3-540-39432-7_63,
	author="Fernando, Chrisantha
	and Sojakka, Sampsa",
	editor="Banzhaf, Wolfgang
	and Ziegler, Jens
	and Christaller, Thomas
	and Dittrich, Peter
	and Kim, Jan T.",
	title="Pattern Recognition in a Bucket",
	booktitle="Advances in Artificial Life",
	year="2003",
	publisher="Springer Berlin Heidelberg",
	address="Berlin, Heidelberg",
	pages="588--597",
	abstract="This paper demonstrates that the waves produced on the surface of water can be used as the medium for a ``Liquid State Machine'' that pre-processes inputs so allowing a simple perceptron to solve the XOR problem and undertake speech recognition. Interference between waves allows non-linear parallel computation upon simultaneous sensory inputs. Temporal patterns of stimulation are converted to spatial patterns of water waves upon which a linear discrimination can be made. Whereas Wolfgang Maass' Liquid State Machine requires fine tuning of the spiking neural network parameters, water has inherent self-organising properties such as strong local interactions, time-dependent spread of activation to distant areas, inherent stability to a wide variety of inputs, and high complexity. Water achieves this ``for free'', and does so without the time-consuming computation required by realistic neural models. An analogy is made between water molecules and neurons in a recurrent neural network.",
	isbn="978-3-540-39432-7"
}


@book{kasabov2019time,
	title={Time-space, spiking neural networks and brain-inspired artificial intelligence},
	author={Kasabov, Nikola K},
	year={2019},
	publisher={Springer}
}

@misc{dan_goodman_2022_7044500,
	author       = {Dan Goodman and
	Tomas Fiers and
	Richard Gao and
	Marcus Ghosh and
	Nicolas Perez},
	title        = {{Spiking Neural Network Models in Neuroscience - 
	Cosyne Tutorial 2022}},
	month        = sep,
	year         = 2022,
	publisher    = {Zenodo},
	version      = {1.0},
	doi          = {10.5281/zenodo.7044500},
	url          = {https://doi.org/10.5281/zenodo.7044500}
}

@misc{jones2020single,
	title={Can Single Neurons Solve MNIST? The Computational Power of Biological Dendritic Trees},
	author={Ilenna Simone Jones and Konrad Paul Kording},
	year={2020},
	eprint={2009.01269},
	archivePrefix={arXiv},
	primaryClass={q-bio.NC},
	url={https://github.com/ilennaj/ktree}
}

@inbook{doi:10.1142/9781848162778_0008,
	author = { Wolfgang   Maass },
	title = {Liquid State Machines: Motivation, Theory, and Applications},
	booktitle = {Computability in Context},
	chapter = {},
	pages = {275-296},
	doi = {10.1142/9781848162778_0008},
	URL = {https://www.worldscientific.com/doi/abs/10.1142/9781848162778_0008},
	eprint = {https://www.worldscientific.com/doi/pdf/10.1142/9781848162778_0008},
	abstract = { Abstract The Liquid State Machine (LSM) has emerged as a computational model that is more adequate than the Turing machine for describing computations in biological networks of neurons. Characteristic features of this new model are (i) that it is a model for adaptive computational systems, (ii) that it provides a method for employing randomly connected circuits, or even “found” physical objects for meaningful computations, (iii) that it provides a theoretical context where heterogeneous, rather than stereo typical, local gates, or processors increase the computational power of a circuit, (iv) that it provides a method for multiplexing different computations (on a common input) within the same circuit. This chapter reviews the motivation for this model, its theoretical background, and current work on implementations of this model in innovative artificial computing devices. }
}