@misc{hasani2020liquid,
	title={Liquid Time-constant Networks}, 
	author={Ramin Hasani and Mathias Lechner and Alexander Amini and Daniela Rus and Radu Grosu},
	year={2020},
	eprint={2006.04439},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@InProceedings{10.1007/978-3-540-39432-7_63,
	author="Fernando, Chrisantha
	and Sojakka, Sampsa",
	editor="Banzhaf, Wolfgang
	and Ziegler, Jens
	and Christaller, Thomas
	and Dittrich, Peter
	and Kim, Jan T.",
	title="Pattern Recognition in a Bucket",
	booktitle="Advances in Artificial Life",
	year="2003",
	publisher="Springer Berlin Heidelberg",
	address="Berlin, Heidelberg",
	pages="588--597",
	abstract="This paper demonstrates that the waves produced on the surface of water can be used as the medium for a ``Liquid State Machine'' that pre-processes inputs so allowing a simple perceptron to solve the XOR problem and undertake speech recognition. Interference between waves allows non-linear parallel computation upon simultaneous sensory inputs. Temporal patterns of stimulation are converted to spatial patterns of water waves upon which a linear discrimination can be made. Whereas Wolfgang Maass' Liquid State Machine requires fine tuning of the spiking neural network parameters, water has inherent self-organising properties such as strong local interactions, time-dependent spread of activation to distant areas, inherent stability to a wide variety of inputs, and high complexity. Water achieves this ``for free'', and does so without the time-consuming computation required by realistic neural models. An analogy is made between water molecules and neurons in a recurrent neural network.",
	isbn="978-3-540-39432-7"
}


@book{kasabov2019time,
	title={Time-space, spiking neural networks and brain-inspired artificial intelligence},
	author={Kasabov, Nikola K},
	year={2019},
	publisher={Springer}
}

@misc{dan_goodman_2022_7044500,
	author       = {Dan Goodman and
	Tomas Fiers and
	Richard Gao and
	Marcus Ghosh and
	Nicolas Perez},
	title        = {{Spiking Neural Network Models in Neuroscience - 
	Cosyne Tutorial 2022}},
	month        = sep,
	year         = 2022,
	publisher    = {Zenodo},
	version      = {1.0},
	doi          = {10.5281/zenodo.7044500},
	url          = {https://doi.org/10.5281/zenodo.7044500}
}

@misc{jones2020single,
	title={Can Single Neurons Solve MNIST? The Computational Power of Biological Dendritic Trees},
	author={Ilenna Simone Jones and Konrad Paul Kording},
	year={2020},
	eprint={2009.01269},
	archivePrefix={arXiv},
	primaryClass={q-bio.NC},
	url={https://arxiv.org/abs/2009.01269}
}

@ARTICLE{6789852,
	author={Maass, Wolfgang and Natschläger, Thomas and Markram, Henry},
	journal={Neural Computation}, 
	title={Real-Time Computing Without Stable States: A New Framework for Neural Computation Based on Perturbations}, 
	year={2002},
	volume={14},
	number={11},
	pages={2531-2560},
	doi={10.1162/089976602760407955}}


@inbook{doi:10.1142/9781848162778_0008,
	author = { Wolfgang   Maass },
	title = {Liquid State Machines: Motivation, Theory, and Applications},
	booktitle = {Computability in Context},
	chapter = {},
	pages = {275-296},
	doi = {10.1142/9781848162778_0008},
	URL = {https://www.worldscientific.com/doi/abs/10.1142/9781848162778_0008},
	eprint = {https://www.worldscientific.com/doi/pdf/10.1142/9781848162778_0008},
	abstract = { Abstract The Liquid State Machine (LSM) has emerged as a computational model that is more adequate than the Turing machine for describing computations in biological networks of neurons. Characteristic features of this new model are (i) that it is a model for adaptive computational systems, (ii) that it provides a method for employing randomly connected circuits, or even “found” physical objects for meaningful computations, (iii) that it provides a theoretical context where heterogeneous, rather than stereo typical, local gates, or processors increase the computational power of a circuit, (iv) that it provides a method for multiplexing different computations (on a common input) within the same circuit. This chapter reviews the motivation for this model, its theoretical background, and current work on implementations of this model in innovative artificial computing devices. }
}

@article{1,
	author = {Jaeger, Herbert},
	year = {2001},
	month = {January},
	pages = {},
	title = {The echo state approach to analysing and training recurrent neural networks-with an erratum note},
	volume = {148},
	journal = {Bonn, Germany: German National Research Center for Information Technology GMD Technical Report}
}

@incollection{2,
	title = {A practical guide to applying echo state networks},
	author = {Mantas Lukosevicius},
	editor = {Grégoire Montavon and  Geneviève B. Orr and Klaus-Robert Müller},
	url = {https://mantas.info/get-publication/?f=Practical_ESN.pdf
	http://link.springer.com/chapter/10.1007%2F978-3-642-35289-8_36
	https://www.ai.rug.nl/minds/uploads/PracticalESN.pdf
	/code/simple_esn/},
	doi = {10.1007/978-3-642-35289-8_36},
	isbn = {978-3-642-35288-1},
	year = {2012},
	date = {2012-01-01},
	booktitle = {Neural Networks: Tricks of the Trade, 2nd Edition},
	volume = {7700},
	pages = {659-686},
	publisher = {Springer},
	series = {LNCS},
	note = {simple source code samples available},
	keywords = {},
	pubstate = {published},
	tppubtype = {incollection}
}

@inproceedings{3,
	title = {Reservoir Computing - Echo State Networks and Liquid State Machines},
	author = {Sankhala, Vikram},
	year = {2023},
	volume = {1},
	ASIN = {B0CCF959T1},
	month = {July}
}

@INPROCEEDINGS{4,
	author={Boccato, Levy and Lopes, Amauri and Attux, Romis and Von Zuben, Fernando José},
	booktitle={The 2011 International Joint Conference on Neural Networks}, 
	title={An echo state network architecture based on volterra filtering and PCA with application to the channel equalization problem}, 
	year={2011},
	volume={},
	number={},
	pages={580-587},
	doi={10.1109/IJCNN.2011.6033273}
}


@article{Hasani2022,
	author = {Hasani, Ramin and Lechner, Mathias and Amini, Alexander and Liebenwein, Lucas and Ray, Aaron and Tschaikowski, Max and Teschl, Gerald and Rus, Daniela},
	title = {Closed-form continuous-time neural networks},
	journal = {Nature Machine Intelligence},
	volume = {4},
	number = {11},
	pages = {992-1003},
	year = {2022},
	month = {November},
	doi = {10.1038/s42256-022-00556-7},
	url = {https://doi.org/10.1038/s42256-022-00556-7},
	issn = {2522-5839},
	date = {2022-11-01},
}
@online{liquid_nn_article,
	author = {Bhaumik Tyagi},
	title = {Liquid Neural Networks: Revolutionizing AI with Dynamic Information Flow},
	year = {2023},
	url = {https://tyagi-bhaumik.medium.com/liquid-neural-networks-revolutionizing-ai-with-dynamic-information-flow-30e27f1cc912},
	note = {Medium article},
}

	