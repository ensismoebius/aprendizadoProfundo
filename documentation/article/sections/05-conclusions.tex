\section{Conclusions}
\label{sec:conclusions}
	\par As seen in the experiments SNN are a plausible alternative to a more energy heavy networks. Although the convergence went from a higher accuracy value to a lower one, overall the network proved to be more stable and, on average, had better performance.
	
	
	But some questions still stand: 
	\par The Surrogate Gradient Descent is a slow method of training, its is borrowed from the standard neural networks and adapted to SNNs. Will there be a training method that fits better for SNNs? 
	\par Another question is how energy efficient the SNNs really are in non-neuromorphic hardware?
	\par Will neuromorphic hardware be easily available for this type of network?
	
\section{Future work}
	\label{sec:future}
	\par Implement SNNs in conjuction with another types of layers like a convolutional layer and apply some prepossessing to data using thecniques like filter banks and wavelets. Some signal processing boards and/or Field-Programmable Gate Arrays (FPGA) may be used to test the real power usage and efficiency of SNNs.